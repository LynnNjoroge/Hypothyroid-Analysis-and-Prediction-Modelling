# -*- coding: utf-8 -*-
"""Moringa_DSCore_IP_Week_8_Lynn_Njoroge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1--ddJrq6NZYYmhAzgjYvEJMNX4ReLDuT

# Defining the Question

### a) Specifying the Question

Build a model that determines whether or not a patient's symptoms indicate that they have hypothyroid.

### b) Defining the Metric for Success

Creating a model that can successfully predict whether a patient's symptoms indicate that they have hypothyroid or not.

### c) Understanding the context

Nairobi Hospital conducted a clinical camp to test for hypothyroidism. The data collected focused on Thyroid patients.

The thyroid is an organ located in the human neck below the Adam's apple. The main purpose of thyroid is to produce thyroid hormones. The produced hormones go through the bloodstream to all the other organs which help to control metabolism and growth development in both adults and children.
The thyroid gland acts as an important part in breathing, blood circulation, bowel movements, temperature of the body, muscle control, digestion, and brain function. An issue with the thyroid gland can result in problems all over the human body. 

The main job of the thyroid gland is to make the hormone thyroxine, also known as T4 because it has four iodine molecules. The thyroid also makes the hormone triiodothyronine, known as T3 because it has three iodine molecules, but in smaller amounts. The thyroid gland makes mostly T4, and the T4 has to be converted to T3, because T3 is the part of thyroxine that actually does the work.

The pituitary gland at the base of the brain controls hormone production in your body. It makes thyroid-stimulating hormone (TSH), which tells the thyroid gland how much T4 and T3 to produce. The TSH level in your blood reveals how much T4 your pituitary gland is asking your thyroid gland to make.

Thyroid diseases are widespread worldwide. Two of the most common thyroid disorders among the oublic are hyperthyroidism and hypothyroidism. 

**Hyperthyroid**-Increase in the hormone production can cause hyperthyroidism. In medical field, “hyper” indicates too
much. Hyperthyroidism crop up when the gland
produces excess hormones

**Hypothyroid**-Decrease in the hormone production can cause hypothyroidism.
In medical field, the term hypo means
deficient or not enough.

For this analysis, we will focus on Hypothyroid. The symptoms of hypothyroid include: Lethargy, reduced heart rate, increased cold sensitivity, numbness
in hands, enlargement in the neck, dry skin and hair,heavy menstrual periods and constipation. 

Hypothyroidism can be diagnosed with thyroid function tests, which measures the levels of Thyroid-Stimulating Hormones
(TSH) in bloodstream of human body. These tests include The T4 test and the TSH test which are the two most common thyroid function tests. They’re usually ordered together. 

A normal range for TSH in most laboratories is 0.4 milliunits per liter (mU/L) to 4.0 mU/L. If your TSH is higher than 4.0 mU/L on repeat tests, you probably have hypothyroidism.

The dataset provided can be found here http://bit.ly/hypothyroid_data 

It contains information about thyroid patients including their age, sex, if they are sick or not, if they have goitre or not, if there is presence of tumors in their bodies or not and their various TSH, T3 and T4 levels including if these hormone levels were measured or not among other features. We will use the inofrmation provided to create a model that can predict if a patient has hypothyroid or not taking into account the features and symptoms provided. 

Fro more info on this, follow the links below:

https://core.ac.uk/download/pdf/231152246.pdf.

https://www.everydayhealth.com/hs/healthy-living-with-hypothyroidism/understanding-test-results/#:~:text=A%20normal%20range%20for%20TSH,t%20get%20into%20your%20cells. 

https://www.everydayhealth.com/hs/healthy-living-with-hypothyroidism/understanding-test-results/#:~:text=A%20normal%20range%20for%20TSH,t%20get%20into%20your%20cells.

### d) Recording the Experimental Design

1. Defining the Question
2. Reading the Data.
3. Checking the Data.
4. Data Cleaning
5. Performing EDA
6. Building Prediction Models
7. Evaluation of the solution
8. Challenging the solution 
9. Conclusion

### e) Data Relevance

This will be discussed after the analysis and prediction has been completed

# Data Preparation

### Importing our Libraries
"""

# installing the necessary libraries not in google colab
! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip

# Commented out IPython magic to ensure Python compatibility.
# Let us first import all the libraries we will need for our analysis
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib
from matplotlib import pyplot as plt
# %matplotlib inline
from pandas_profiling import ProfileReport
from scipy import stats
from scipy.stats import norm
import math
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, cross_val_score
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score

# let us set the warnings that may appear in our analysis off

import warnings
warnings.filterwarnings('ignore') 

# subsequently let us set the pandas warning for chained assignments off
pd.options.mode.chained_assignment = None  # default='warn'

"""### Loading and Previewing our Dataset"""

# Loading the Dataset from the source i.e. csv
df = pd.read_csv('/content/hypothyroid.csv')
df.head()

"""## Checking the Data"""

# Determining the no. of records in our dataset
#
print('This hypothyroid dataset has ' + str(df.shape[0]) + ' rows, and ' + str(df.shape[1]) + ' columns')

# Checking whether each column has an appropriate datatype
#
df.dtypes

# checking the dataset information
df.info()

# let us see the columns in our dataframe
df.columns

# Checking the entire profile of the dataframe

profile = ProfileReport(df, title="Hypothyroid Profile Report", html={'style':{'full_width':True}})
profile.to_notebook_iframe()

# let us save our profile report
profile.to_file(output_file="Hypothyroid_Profile_Report.html")

# let us see the unique values of all our columns
#for col in df:
 #   print(df[col].unique())
 #df.describe(include='all').loc['unique', :]
for col in list(df):
    print(col)
    print(df[col].unique())

"""# Data Cleaning"""

# let us make a copy of our dataset to clean and do modelling on
df_copy = df.copy(deep = True)

# For consistency, the column names should be uniform
# Let us change all column names to lower case
#
df_copy.columns = df_copy.columns.str.lower()
# Previewing the columns to confirm whether the changes have taken effect
#
df_copy.columns

# From our profile report, we can see that we have any duplicated rows
# But let us check 
df_copy.duplicated().sum()

# let us see these duplicated rows
df_copy[df_copy.duplicated()]
# we will not drop the duplicated columns as they have different data in most of the columns

#We now replace '?' values in dataset with NaN values so that it becomes easy to count them
df_copy.replace(to_replace='?', inplace=True, value=np.NaN)

# Let Us Drop the TBG column since it is mostly null values and the TBG measures

df_copy.drop(['tbg', 'tbg_measured'], axis = 1, inplace = True)

# let us confirm that we have dropped the unnecessary columns
df_copy.head()

# let us fix the datatypes in our dataframe
categorical_columns = ['sex', 'on_thyroxine','query_on_thyroxine','on_antithyroid_medication',
                       'thyroid_surgery','query_hypothyroid','query_hyperthyroid', 'pregnant',
                       'sick','tumor', 'lithium', 'goitre', 'tsh_measured', 't3_measured', 
                       'tt4_measured', 't4U_measured', 'fti_measured']

# Numerical columns array 
numerical_columns = ['age','tsh','t3', 'tt4', 't4u', 'fti']


# Using a conditional for loop to convert each columns into the right data types

for column in df_copy.columns:
  if column in numerical_columns:
    df_copy[column] = pd.to_numeric(df_copy[column])
  else:
    df_copy[column] = df_copy[column].astype('category')

# Confirming changes
df_copy.dtypes
# The data types have now been set correctly

# let us check for missng values in our dataframe
df_copy.isnull().sum()

# let us see how much information we lose by dropping the null values
df_clean = df_copy.dropna()
print('Shape of dataframe with no missing values: ', df_clean.shape)
print('Shape of dataframe with missing values: ', df_copy.shape)
# by dropping our null values we have dropped 1000 rows of our dataframe
# this is a huge part of our dataset which is not ideal
# therefore we will need to impute our missing values

# List of columns containing null values
df_copy.columns[df_copy.isnull().any()].tolist()

# to see the appropriate way to impute missing values 
# let us plot histograms to visualize patterns in the data
df_copy.hist(figsize = (10,10))

# The features "age" and "t4u" show a normal distribution, 
# which is why we will be replacing the missing values with mean.
df_copy['age'].fillna(df_copy['age'].mean(), inplace = True)
df_copy['t4u'].fillna(df_copy['t4u'].mean(), inplace = True)

#The features "TSH", "T3", "TT4" and "FTI" show a skewed distribution,
#for continuous data, the use of the median is better than mean since it is not influenced by outliers
#which is why we will be replacing the missing values with median.
df_copy['tsh'].fillna(df_copy['tsh'].mean(), inplace = True)
df_copy['t3'].fillna(df_copy['t3'].median(), inplace = True)
df_copy['tt4'].fillna(df_copy['tt4'].median(), inplace = True)
df_copy['fti'].fillna(df_copy['fti'].median(), inplace = True)

#The feature "gender" is catgorical and contains imbalanced data 
#with the values "f" much greater then "m". 
#for categorical data using mode makes more sense 
#So we will be replacing the missing values with "f".

df_copy['sex'].fillna('F', inplace = True)

# let us see if we have successfully filled all missing values
df_copy.isnull().sum()

"""# Exploratory data Analysis

### Univariate Analysis
"""

# finding the information about the variables
df_copy.info()

# Boxplots to Visualize outliers of our numerical columns 
plt.figure(figsize = (20,10))
ax = sns.boxplot(data=df_copy, orient="v", palette="Set2")
plt.title('Checking for outliers using boxplots')
# The boxplots below indicate the  outliers in each of the numerical columns

# let us check the distributions of our data
# 
df_copy.hist(bins=30, figsize=(20, 7))

# age and t4u features are normally distributed
# tsh, t3, tt4 and fti are skewed to the righ 
# meaning that the mean is greater than the mode

# let us use the measures of central tendency to prove the skewness of the
# histograms above
# tsh
print('The mean of tsh column is', df_copy["tsh"].mean())
print('The mode of tsh column is', df_copy["tsh"].mode())
# t3
print('The mean of t3 column is', df_copy["t3"].mean())
print('The mode of t3 column is', df_copy["t3"].mode())
# tt4
print('The mean of tt4 column is', df_copy["tt4"].mean())
print('The mode of tt4 column is', df_copy["tt4"].mode())
# fti
print('The mean of fti column is', df_copy["fti"].mean())
print('The mode of fti column is', df_copy["fti"].mode())

# in all column instances mentioned above, the mean is greater than the mode,
# which justifies the right skewness of our histograms above

# Bar graph showing status count of patients in our dataframe 

df_copy.status.value_counts().sort_values().plot.bar(color='Purple')
plt.title('Status Count', color='red')
degrees = 0
plt.xticks(rotation=degrees)

# from this we can see that most patients do not have hypothyroid

# Bar graph showing count of males and females in our dataframe 
df_copy.sex.value_counts().sort_values().plot.barh()
plt.title('Gender Count')
# from our bargraph, we can see that most of the patients are female

# Bar graph showing count of pregnant women in our dataframe 
df_copy.pregnant.value_counts().sort_values().plot.bar(color='green')
plt.title('Gender Count')
degrees = 60
plt.xticks(rotation=degrees)
# from the graph we can see that a high number of the patients are not pregnant

# pie chart visualizing sick patients
df_copy.sick.value_counts().plot(kind= 'pie', figsize=[10,10], autopct = '%1.1f%%')
plt.title('A Pie chart of Sick Patients') 
# from the pie chart, we can see that a great percentage of patients are classified as not being sick

# Bar graph showing count of patients with tumors in our dataframe 
df_copy.tumor.value_counts().sort_values().plot.barh(color='cyan')
plt.title('Count of patients with tumors')

# from the graph we can see that a high number of the patients do not have tumors

"""# Bivariate Analysis"""

# Ploting the bivariate summaries and recording our observations
sns.pairplot(df_copy, hue="sex")
plt.show()

# Calculating the pearson coefficient correlation
a = df_copy.corr() 
plt.figure(figsize = (20,10))
sns.heatmap(a, xticklabels=a.columns, yticklabels=a.columns, annot=True)
plt.title('A Heatmap of Pearson Correlation in our Dataset', color='red')
plt.show()
# From this we can see there is a correlation between t3 and tt4.

"""### EDA Conclusion

From the EDA performed above we can conclude that:

1. Age and t4u features are normally distributed while tsh, t3, tt4 and fti are skewed to the right meaning that the mean is greater than the mode. 
2. Most of the patients are female
3. Most of the female patients are not pregnant
4. Most of the patients did not register sick as one of their symptoms
5. Most of the patients do not have tumors
6. There is a significant positive correlation between t3 and tt4 features.
7. There is a significant positive relationship between tt4 and fti features.
8. There is also a positive relationship between t3 and t4u and fti features.

# Prediction Models

### Data Pre-Processing
"""

# label encoding of categorical data
#
df_copy = df_copy.replace({"t":1,"f":0, "y":1, "n":0, "hypothyroid":1, "negative":0, "F":1, "M":0})
# let us preview our dataset to ensure the changes have taken effect
df_copy.head()

# let us viwe the data types of our dataframe to make sure they are all numerical now
df_copy.info()

"""Before we begin our modelling process, let us define the performance metrics we will use to evaluate how good our models are.

Precision: It is implied as the measure of the correctly identified positive cases from all the predicted positive cases. Thus, it is useful when the costs of False Positives is high

Recall: It is the measure of the correctly identified positive cases from all the actual positive cases. It is important when the cost of False Negatives is high.

Accuracy: One of the more obvious metrics, it is the measure of all the correctly identified cases. It is most used when all the classes are equally important.

F1-score: This is the harmonic mean of Precision and Recall and gives a better measure of the incorrectly classified cases than the Accuracy Metric.

For this analysis, we will focus on F1score since it is a better metric to evaluate our models on.

NB: In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric to evaluate our model on.

NB: Test accuracy should not be higher than train since the model is optimized for the latter. We will test both train and test data to make sure this remains true.

# Random Forest Classifier

### Random Forest
"""

# Split the independent and dependent variables
X = df_copy.drop('status', axis =1)
y = df_copy.status

# Train using 70% of the data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Performing standard scalar normalization to normalize our feature set.
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train =sc.fit_transform(X_train)
X_test = sc.transform (X_test)

# Instantiating the model
forest = RandomForestClassifier(n_estimators = 100, random_state = 42,
                                min_samples_split = 10, max_depth=5)
forest.fit(X_train, y_train)

# Making predictions for our test dataset
y_pred = forest.predict(X_test)

# Measuring the accuracy of the model on the test dataset
print(f'The accuracy score for our test dataset is: {accuracy_score(y_test, y_pred)}')
print(f'The f1 score for our test dataset is {f1_score(y_test, y_pred)}')
print(confusion_matrix(y_test, y_pred))

# Making predictions for our train dataset
y_pred_train = forest.predict(X_train)

# Measuring the accuracy of the model on the train dataset
print('\n')
print(f'The accuracy score for our train dataset is: {accuracy_score(y_train, y_pred_train)}')
print(f'The f1 score for our train dataset is {f1_score(y_train, y_pred_train)}')
print(confusion_matrix(y_train, y_pred_train))

from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO  
from IPython.display import Image  
import pydotplus

dot_data = StringIO()
# pick a specific tree from the forest to visualize
tree = forest.estimators_[40]

export_graphviz(tree, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True,feature_names = X.columns)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('hypothyroid_forest.png')
Image(graph.create_png())

# Getting the feature importance
# Creating a dataframe of features and their respective importances
# plotting a bar graph to visualize the important features of our random forest
pd.DataFrame({'Features' : X.columns, 'Importance' : forest.feature_importances_})\
.sort_values(by = 'Importance', ascending = True).set_index('Features')\
.plot.barh(title = 'Feature Importance for Random Forests',figsize = (10, 8), color = 'purple')
plt.show()

# let us see the 10 most important features of or dataframe
pd.DataFrame({'Features' : X.columns, 'Importance' : forest.feature_importances_})\
.sort_values(by = 'Importance', ascending = False).set_index('Features').head(10)

"""### Remodelling with Most Important features"""

# let us do some remodelling with the top 10 important features to see if our f1 score will improve
X = df_copy[['fti', 'tsh', 'tt4', 't3','t4u', 
             'age', 'tsh_measured', 'on_antithyroid_medication', 'thyroid_surgery', 'query_hypothyroid']]
y = df_copy.status

# Train using 70% of the data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Performing standard scalar normalization to normalize our feature set.
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train =sc.fit_transform(X_train)
X_test = sc.transform (X_test)

# Instantiating the model
forest = RandomForestClassifier(n_estimators = 100, random_state = 42,
                                min_samples_split = 10, max_depth=5)
forest.fit(X_train, y_train)

# Making predictions for our test dataset
y_pred = forest.predict(X_test)

# Measuring the accuracy of the model on the test dataset
print(f'The accuracy score for our test dataset is: {accuracy_score(y_test, y_pred)}')
print(f'The f1 score for our test dataset is {f1_score(y_test, y_pred)}')
print(confusion_matrix(y_test, y_pred))

# Making predictions for our train dataset
y_pred_train = forest.predict(X_train)

# Measuring the accuracy of the model on the train dataset
print('\n')
print(f'The accuracy score for our train dataset is: {accuracy_score(y_train, y_pred_train)}')
print(f'The f1 score for our train dataset is {f1_score(y_train, y_pred_train)}')
print(confusion_matrix(y_train, y_pred_train))

# using 10 of the most important features highlihted by the random forest
# has resulted in a significant increase of our f1 score

"""### Hyperparameter Tuning """

# let us see what our optimal parameters for our random forest are

# Look at parameters used by our current forest
rf = RandomForestClassifier(random_state = 42)
from pprint import pprint
# Look at parameters used by our current forest
print('Parameters currently in use:\n')
pprint(rf.get_params())

#We will try adjusting the following set of hyperparameters:
#n_estimators = number of trees in the foreset
#max_depth = max number of levels in each decision tree
#min_samples_split = min number of data points placed in a node before the node is split

from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 10, stop = 1000, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [5,10,15,20]
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]

# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split}

# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf = RandomForestClassifier()
# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations, and use all available cores
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, 
                               n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
# Fit the random search model
rf_random.fit(X, y)
# Checking for the best parameters
#
print(f'The best parameters are: {rf_random.best_params_}')

# Applying the best parameters to the model
# Selecting only important features and the y variable
X = df_copy[['fti', 'tsh', 'tt4', 't3','t4u', 
             'age', 'tsh_measured', 'on_antithyroid_medication', 'thyroid_surgery', 'query_hypothyroid']]
y = df_copy.status

# Train using 70% of the data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Performing standard scalar normalization to normalize our feature set.
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train =sc.fit_transform(X_train)
X_test = sc.transform (X_test)

# Instantiating the model
forest = RandomForestClassifier(n_estimators = 670, random_state = 42,
                                min_samples_split = 10, max_depth=20)
forest.fit(X_train, y_train)

# Making predictions for our test dataset
y_pred = forest.predict(X_test)

# Measuring the accuracy of the model on the test dataset
print(f'The accuracy score for our test dataset is: {accuracy_score(y_test, y_pred)}')
print(f'The f1 score for our test dataset is {f1_score(y_test, y_pred)}')
print(confusion_matrix(y_test, y_pred))

# Making predictions for our train dataset
y_pred_train = forest.predict(X_train)

# Measuring the accuracy of the model on the train dataset
print('\n')
print(f'The accuracy score for our train dataset is: {accuracy_score(y_train, y_pred_train)}')
print(f'The f1 score for our train dataset is {f1_score(y_train, y_pred_train)}')
print(confusion_matrix(y_train, y_pred_train))

## Applying the best parameters to the model 
#Split the independent and dependent variables
# let us use all the features
X = df_copy.drop('status', axis =1)
y = df_copy.status

# Train using 70% of the data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Performing standard scalar normalization to normalize our feature set.
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train =sc.fit_transform(X_train)
X_test = sc.transform (X_test)

# Instantiating the model
forest = RandomForestClassifier(n_estimators = 670, random_state = 42,
                                min_samples_split = 10, max_depth=20)
forest.fit(X_train, y_train)

# Making predictions for our test dataset
y_pred = forest.predict(X_test)

# Measuring the accuracy of the model on the test dataset
print(f'The accuracy score for our test dataset is: {accuracy_score(y_test, y_pred)}')
print(f'The f1 score for our test dataset is {f1_score(y_test, y_pred)}')
print(confusion_matrix(y_test, y_pred))

# Making predictions for our train dataset
y_pred_train = forest.predict(X_train)

# Measuring the accuracy of the model on the train dataset
print('\n')
print(f'The accuracy score for our train dataset is: {accuracy_score(y_train, y_pred_train)}')
print(f'The f1 score for our train dataset is {f1_score(y_train, y_pred_train)}')
print(confusion_matrix(y_train, y_pred_train))

"""Let us compare all the f1 scores of the models we build using random froest classifier:

Our normal random forest classifier:
*   The f1 score for our test dataset is 0.7536
*   The f1 score for our train dataset is 0.9004

Our model with 10 most important features:
*  The f1 score for our test dataset is 0.8607
*  The f1 score for our train dataset is 0.9511

Our model with most important features and using optimal parameters from random search:
*   The f1 score for our test dataset is 0.8607
*   The f1 score for our train dataset is 0.9686

Our model with all features and using optimal parameters from random search:


*   The f1 score for our test dataset is 0.8157
*   The f1 score for our train dataset is 0.9642

# Gradient Boosted Trees
"""

# Split the independent and dependent variables
# let us use all the features
X = df_copy.drop('status', axis =1)
y = df_copy.status

# Train using 70% of the data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Performing standard scalar normalization to normalize our feature set.
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train =sc.fit_transform(X_train)
X_test = sc.transform (X_test)

# Instantiating the model
gbc = GradientBoostingClassifier(loss='deviance',n_estimators=200, learning_rate=0.01, max_depth=5)
gbc = gbc.fit(X_train, y_train)

# Making predictions for our test dataset
y_pred_gbc = gbc.predict(X_test)

# Measuring the accuracy of the model on the test dataset
print(f'The accuracy score for our test dataset is: {accuracy_score(y_test, y_pred_gbc)}')
print(f'The f1 score for our test dataset is {f1_score(y_test, y_pred_gbc)}')
print(confusion_matrix(y_test, y_pred_gbc))

# Making predictions for our train dataset
y_pred_gbc_train = gbc.predict(X_train)

# Measuring the accuracy of the model on the train dataset
print('\n')
print(f'The accuracy score for our train dataset is: {accuracy_score(y_train, y_pred_gbc_train)}')
print(f'The f1 score for our train dataset is {f1_score(y_train, y_pred_gbc_train)}')
print(confusion_matrix(y_train, y_pred_gbc_train))

# Getting the feature importance
# Creating a dataframe of features and their respective importances
# plotting a bar graph to visualize the important features of our random forest
pd.DataFrame({'Features' : X.columns, 'Importance' : gbc.feature_importances_})\
.sort_values(by = 'Importance', ascending = True).set_index('Features')\
.plot.barh(title = 'Feature Importance for Random Forests',figsize = (10, 8), color = 'purple')
plt.show()

# let us see the 10 most important features of or dataframe
pd.DataFrame({'Features' : X.columns, 'Importance' : gbc.feature_importances_})\
.sort_values(by = 'Importance', ascending = False).set_index('Features').head(10)

"""### Remodelling using 10 most Important Features"""

# let us do some remodelling with the top 10 important features to see if our f1 score will improve
# Split the independent and dependent variables
# let us use 10 of the features
X = df_copy[['fti',	'tsh','on_antithyroid_medication','age','thyroid_surgery',
             't4u','tt4','t3','sex',	'on_thyroxine']]
y = df_copy.status

# Train using 70% of the data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Performing standard scalar normalization to normalize our feature set.
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train =sc.fit_transform(X_train)
X_test = sc.transform (X_test)

# Instantiating the model
gbc = GradientBoostingClassifier(loss='deviance',n_estimators=300, learning_rate=0.1, max_depth=5)
gbc = gbc.fit(X_train, y_train)

# Making predictions for our test dataset
y_pred_gbc = gbc.predict(X_test)

# Measuring the accuracy of the model on the test dataset
print(f'The accuracy score for our test dataset is: {accuracy_score(y_test, y_pred_gbc)}')
print(f'The f1 score for our test dataset is {f1_score(y_test, y_pred_gbc)}')
print(confusion_matrix(y_test, y_pred_gbc))

# Making predictions for our train dataset
y_pred_gbc_train = gbc.predict(X_train)

# Measuring the accuracy of the model on the train dataset
print('\n')
print(f'The accuracy score for our train dataset is: {accuracy_score(y_train, y_pred_gbc_train)}')
print(f'The f1 score for our train dataset is {f1_score(y_train, y_pred_gbc_train)}')
print(confusion_matrix(y_train, y_pred_gbc_train))

# using only 10 features yield a more accurate model than using all features



"""### Hyperparameter Tuning """

# let us see what our optimal parameters for our random forest are

# Look at parameters used by our current forest
gradient = GradientBoostingClassifier(random_state = 42)
# Look at parameters used by our current forest
print('Parameters currently in use:\n')
pprint(gradient.get_params())

#We will try adjusting the following set of hyperparameters:
#n_estimators = number of trees in the foreset
#max_depth = max number of levels in each decision tree
#min_samples_split = min number of data points placed in a node before the node is split
#learning rate: shrinks the contribution of each tree

from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 10, stop = 1000, num = 10)]
# Maximum number of levels in tree
max_depth = [5,10,15,20]
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# learning rate
learning_rate = [1, 0.5, 0.25, 0.1, 0.05, 0.01]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'learning_rate': learning_rate}

# Use the random grid to search for best hyperparameters
# First create the base model to tune
gbc = GradientBoostingClassifier()
# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations, and use all available cores
gbc_random = RandomizedSearchCV(estimator = gbc, param_distributions = random_grid, 
                               n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
# Fit the random search model
gbc_random.fit(X, y)
# Checking for the best parameters
#
print(f'The best parameters are: {gbc_random.best_params_}')

# Applying the best parameters to the model
# Selecting only important features and the y variable
X = df_copy[['fti',	'tsh','on_antithyroid_medication','age','thyroid_surgery',
             't4u','tt4','t3','sex',	'on_thyroxine']]
y = df_copy.status

# Train using 70% of the data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Performing standard scalar normalization to normalize our feature set.
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train =sc.fit_transform(X_train)
X_test = sc.transform (X_test)

# Instantiating the model
gbc = GradientBoostingClassifier(loss='deviance',n_estimators=120, 
                                 learning_rate=0.5, max_depth=5, min_samples_split=5)
gbc = gbc.fit(X_train, y_train)

# Making predictions for our test dataset
y_pred_gbc = gbc.predict(X_test)

# Measuring the accuracy of the model on the test dataset
print(f'The accuracy score for our test dataset is: {accuracy_score(y_test, y_pred_gbc)}')
print(f'The f1 score for our test dataset is {f1_score(y_test, y_pred_gbc)}')
print(confusion_matrix(y_test, y_pred_gbc))

# Making predictions for our train dataset
y_pred_gbc_train = gbc.predict(X_train)

# Measuring the accuracy of the model on the train dataset
print('\n')
print(f'The accuracy score for our train dataset is: {accuracy_score(y_train, y_pred_gbc_train)}')
print(f'The f1 score for our train dataset is {f1_score(y_train, y_pred_gbc_train)}')
print(confusion_matrix(y_train, y_pred_gbc_train))

# # Applying the best parameters to the model
# let us use all the features
X = df_copy.drop('status', axis =1)
y = df_copy.status

# Train using 70% of the data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Performing standard scalar normalization to normalize our feature set.
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train =sc.fit_transform(X_train)
X_test = sc.transform (X_test)

# Instantiating the model
gbc = GradientBoostingClassifier(loss='deviance',n_estimators=120, 
                                 learning_rate=0.5, max_depth=5, min_samples_split=5)
gbc = gbc.fit(X_train, y_train)

# Making predictions for our test dataset
y_pred_gbc = gbc.predict(X_test)

# Measuring the accuracy of the model on the test dataset
print(f'The accuracy score for our test dataset is: {accuracy_score(y_test, y_pred_gbc)}')
print(f'The f1 score for our test dataset is {f1_score(y_test, y_pred_gbc)}')
print(confusion_matrix(y_test, y_pred_gbc))

# Making predictions for our train dataset
y_pred_gbc_train = gbc.predict(X_train)

# Measuring the accuracy of the model on the train dataset
print('\n')
print(f'The accuracy score for our train dataset is: {accuracy_score(y_train, y_pred_gbc_train)}')
print(f'The f1 score for our train dataset is {f1_score(y_train, y_pred_gbc_train)}')
print(confusion_matrix(y_train, y_pred_gbc_train))

"""Let us compare all the f1 scores of the models we build using gradient boosting classifier:

Our normal gradient boost classifier:
*   The f1 score for our test dataset is 0.8837
*   The f1 score for our train dataset is 0.9857

Our model with 10 most important features:
* The f1 score for our test dataset is 0.9213
* The f1 score for our train dataset is 0.9952

Our model with most important features and using optimal parameters from random search:
*  The f1 score for our test dataset is 0.8636
*  The f1 score for our train dataset is 0.9953

Our model with all features and using optimal parameters from random search:
*  The f1 score for our test dataset is 0.8764
*  The f1 score for our train dataset is 1.0

# Decision trees Conclusion

Comparing the metrics of Random Forest and Gradient Boosting, The Gradient Boosting Model performs the best.

# SVM (Support Vector Machines)
"""

# for SVM we will use the two most important features identified
# by both random forest and gradient boost

# let us use all the features
X = df_copy[['fti', 'tsh']]
y = df_copy.status

# Train using 70% of the data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardising the data
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

## Creating the linear kernel
# Fit the model
linear = SVC(kernel= 'linear')
linear.fit(X_train, y_train)

# Making predictions for our test dataset
y_pred_linear = linear.predict(X_test)

# Measuring the accuracy of the model on the test dataset
print(f'The accuracy score for our test dataset is: {accuracy_score(y_test, y_pred_linear)}')
print(f'The f1 score for our test dataset is {f1_score(y_test, y_pred_linear)}')
print(confusion_matrix(y_test, y_pred_linear))

# Making predictions for our train dataset
y_pred_linear_train = linear.predict(X_train)

# Measuring the accuracy of the model on the train dataset
print('\n')
print(f'The accuracy score for our train dataset is: {accuracy_score(y_train, y_pred_linear_train)}')
print(f'The f1 score for our train dataset is {f1_score(y_train, y_pred_linear_train)}')
print(confusion_matrix(y_train, y_pred_linear_train))

## Creating the polynomial kernel
# Fit the model
poly = SVC(kernel= 'poly')
poly.fit(X_train, y_train)

# Making predictions for our test dataset
y_pred_poly = poly.predict(X_test)

# Measuring the accuracy of the model on the test dataset
print(f'The accuracy score for our test dataset is: {accuracy_score(y_test, y_pred_poly)}')
print(f'The f1 score for our test dataset is {f1_score(y_test, y_pred_poly)}')
print(confusion_matrix(y_test, y_pred_poly))

# Making predictions for our train dataset
y_pred_poly_train = poly.predict(X_train)

# Measuring the accuracy of the model on the train dataset
print('\n')
print(f'The accuracy score for our train dataset is: {accuracy_score(y_train, y_pred_poly_train)}')
print(f'The f1 score for our train dataset is {f1_score(y_train, y_pred_poly_train)}')
print(confusion_matrix(y_train, y_pred_poly_train))

## Creating the rbf kernel
# Fit the model
rbf = SVC(kernel= 'rbf')
rbf.fit(X_train, y_train)

# Making predictions for our test dataset
y_pred_rbf = rbf.predict(X_test)

# Measuring the accuracy of the model on the test dataset
print(f'The accuracy score for our test dataset is: {accuracy_score(y_test, y_pred_rbf)}')
print(f'The f1 score for our test dataset is {f1_score(y_test, y_pred_rbf)}')
print(confusion_matrix(y_test, y_pred_rbf))

# Making predictions for our train dataset
y_pred_rbf_train = rbf.predict(X_train)

# Measuring the accuracy of the model on the train dataset
print('\n')
print(f'The accuracy score for our train dataset is: {accuracy_score(y_train, y_pred_rbf_train)}')
print(f'The f1 score for our train dataset is {f1_score(y_train, y_pred_rbf_train)}')
print(confusion_matrix(y_train, y_pred_rbf_train))

# our best model based on f1 score is the rbf svm kernel

# Previewing parameters of the SVC model that can be tuned
#
SVC()

"""### Hyperparameter Tuning """

# let us see what our optimal parameters for SVm are
SVC()

# defining parameter range 
param_grid = {'C': [0.1, 1, 10, 100, 1000],  
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 
              'kernel': ['rbf']}  
  
grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)

#Fit the model for grid search
grid.fit(X_train, y_train)

#Now find the best parameters.
print(grid.best_params_)
print(grid.best_estimator_)

# Applying the best parameters to the model
# Selecting all features
X = df_copy.drop('status', axis=1)
y = df_copy.status

# Train using 70% of the data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardising the data
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

## Creating the rbf kernel
# Fit the model
rbf = SVC(kernel= 'rbf', C=100, gamma=0.001, degree=3)
rbf.fit(X_train, y_train)

# Making predictions for our test dataset
y_pred_rbf = rbf.predict(X_test)

# Measuring the accuracy of the model on the test dataset
print(f'The accuracy score for our test dataset is: {accuracy_score(y_test, y_pred_rbf)}')
print(f'The f1 score for our test dataset is {f1_score(y_test, y_pred_rbf)}')
print(confusion_matrix(y_test, y_pred_rbf))

# Making predictions for our train dataset
y_pred_rbf_train = rbf.predict(X_train)

# Measuring the accuracy of the model on the train dataset
print('\n')
print(f'The accuracy score for our train dataset is: {accuracy_score(y_train, y_pred_rbf_train)}')
print(f'The f1 score for our train dataset is {f1_score(y_train, y_pred_rbf_train)}')
print(confusion_matrix(y_train, y_pred_rbf_train))

# with the best parmas applied and all features used,
# our f1 score has reduced significantly

"""# Prediction models Conclusion

Of all the prediction models we have build so far, the gradient boosted classifier performs the best.

# Challenging the Solution
"""

# compare algorithms
from matplotlib import pyplot
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

# Declaring our X and y variables
X = df_copy.drop('status', axis=1)
y = df_copy.status

# Splitting the data into training and test sets,
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Spot Check Algorithms
models = []
models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('RF', RandomForestClassifier()))
models.append(('GBC', GradientBoostingClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC(gamma='auto')))
# evaluate each model in turn
# evaluate each model in turn
results = []
names = []
for name, model in models:
	kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)
	cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')
	results.append(cv_results)
	names.append(name)
	print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))

# from the above results, 
# we can see that the Gradient Boosted Classifier is the best
# among all the other models
# which suports the conclusion we had come to in our modelling process

"""# Follow up questions

a). Did we have the right data? 

YES we had the right data to carry out our analysis and modelling

b). Do we need other data to answer our question? 

NO the data we had is sufficient

c). Did we have the right question? 

YES we had the right question
"""